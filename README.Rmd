---
output:
  github_document:
    html_preview: true
  html_document:
    keep_md: yes
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

<!-- README.Rmd generates README.md. -->

```{r, echo=FALSE}
knitr::opts_chunk$set(
  # collapse = TRUE,
  fig.align = "center",
  comment = "#>",
  fig.path = "man/figures/",
  message = FALSE,
  warning = FALSE
)

options(width = 120)
```


# `{tweetio}`


<!-- badges: start -->
[![Lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/knapply/tweetio?branch=master&svg=true)](https://ci.appveyor.com/project/knapply/tweetio)
[![Travis-CI Build Status](https://travis-ci.org/knapply/tweetio.svg?branch=master)](https://travis-ci.org/knapply/tweetio)
[![Codecov test coverage](https://codecov.io/gh/knapply/tweetio/branch/master/graph/badge.svg)](https://codecov.io/gh/knapply/tweetio?branch=master)
[![GitHub last commit](https://img.shields.io/github/last-commit/knapply/tweetio.svg)](https://github.com/knapply/tweetio/commits/master)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![Depends](https://img.shields.io/badge/Depends-GNU_R>=3.3-blue.svg)](https://www.r-project.org/)
[![CRAN status](https://www.r-pkg.org/badges/version/tweetio)](https://cran.r-project.org/package=tweetio)
[![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/knapply/tweetio.svg)](https://github.com/knapply/tweetio)
[![HitCount](http://hits.dwyl.io/knapply/tweetio.svg)](http://hits.dwyl.io/knapply/tweetio)
<!-- badges: end -->

<!-- [![R build status](https://github.com/knapply/tweetio/workflows/R-CMD-check/badge.svg)](https://github.com/knapply/tweetio/actions?workflow=R-CMD-check) -->




# Introduction

`{tweetio}`'s goal is to enable safe, efficient I/O and transformation of Twitter data. Whether the data came from the Twitter API, a database dump, or some other source, `{tweetio}`'s job is to get them into R and ready for analysis.

`{tweetio}` is __not__ a competitor to [`{rtweet}`](https://rtweet.info/): it is not interested in collecting Twitter data. That said, it definitely attempts to compliment it by emulating its data frame schema because...

1. It's incredibly easy to use.
2. It's more efficient to analyze than a key-value format following the raw data.
3. It'd be a waste not to maximize compatibility with tools built specifically around `{rtweet}`'s data frames.

# Installation

You'll need a C++ compiler. If you're using Windows, that means [Rtools](https://cran.r-project.org/bin/windows/Rtools/).

```{r, eval=FALSE}
# install.packages("remotes")
remotes::install_github("knapply/tweetio")
```


# Usage

```{r}
library(tweetio)
```

## Simple Example

First, we'll save a stream of tweets using `rtweet::stream_tweets()`.

```{r}
temp_file <- tempfile(fileext = ".json")
rtweet::stream_tweets(timeout = 15, parse = FALSE, file_name = temp_file)
```

We can then pass the file path to `tweetio::read_tweets()` to efficiently parse the data into an `{rtweet}`-style data frame.

```{r}
small_rtweet_stream <- read_tweets(temp_file, as_tibble = TRUE)

small_rtweet_stream
```

## Scaling Up

We're more interested in handling much larger data sets, but for demonstration we'll use a file of a reasonable size that was obtained using `rtweet::stream_tweets()`. 

```{r}
rtweet_stream_path <- "inst/example-data/api-stream.json.gz"

rtweet_stream_path %>% 
  file.size() %>% 
  scales::number_bytes() # 153 Mb before compression
```


Unfortunately, `rtweet::parse_stream()` may fail parsing streams because the data returned may not be valid JSON.

```{r, error=TRUE}
rtweet::parse_stream(rtweet_stream_path)
```

The only way around this is to read the entire file into memory and run some validation routine before parsing, which looks something like this...

```{r}
rtweet_validate_and_parse <- function(source_file_path, target_file_path) {
  raw_lines <- readLines(source_file_path)
  valid_lines <- raw_lines[vapply(raw_lines, jsonify::validate_json, logical(1L))]
  writeLines(valid_lines, target_file_path)
  
  rtweet::parse_stream(target_file_path)
}

rtweet_stream_path2 <- tempfile(fileext = ".json")

rtweet_validate_and_parse(source_file_path = rtweet_stream_path, 
                          target_file_path = rtweet_stream_path2)
```


Fortunately, `{tweetio}` can handle these situations by validating the JSON before it gets anywhere near R.

```{r}
bench::mark(
  read_tweets(rtweet_stream_path),
  min_iterations = 3
)[, 1:9]
```


Ignoring the time it takes to run the above `rtweet_validate_and_parse()` function for `rtweet::parse_stream()` to work, how much faster is `tweetio::read_tweets()`?

```{r, out.width="100%"}
bench_mark <- bench::mark(
  rtweet = rtweet::parse_stream(rtweet_stream_path2),
  tweetio = tweetio::read_tweets(rtweet_stream_path2),
  check = FALSE,
  min_iterations = 3,
  filter_gc = FALSE
)

bench_mark[, 1:9]

plot(bench_mark)
```


With bigger files, using `rtweet::parse_stream()` is no longer realistic, but that's where `tweetio::read_tweets()` can help.

```{r}
big_tweet_stream_path <- "~/ufc-tweet-stream.json.gz"
big_tweet_stream_path %>%
  file.size() %>% 
  scales::number_bytes() # 1.2 Gb before compression

bench::mark(
  big_tweet_df <- read_tweets(big_tweet_stream_path)
)[, 1:9]
```



## Data Dumps

A common practice for handling social media data at scale is to store them in search engine databases like Elasticsearch, but it's (unfortunately) possible that you'll need to work with data dumps.

The data collected from APIs are stored inside a nested JSON object named `"doc"`. `"doc"` is typically embedded in a ton of system metadata, most of which you probably don't care about.

I've encountered two flavors of these schema:

1. .jsonl: newline-delimited JSON
2. .json: the complete contents of a database dump packed in a JSON array

The .jsonl structure looks something like this:


```json
{"system_metadata_you_dont_care_about":"blahblahblah","more_metadata1":"blahblahblah","more_metadata2":"blahblahblah","more_metadata3":"blahblahblah","more_metadata4":"blahblahblah","more_metadata5":"blahblahblah","doc":{"text":"********************HERE'S THE DATA YOU ACTUALLY WANT********************","id":92108498098018010401,"id_str":"92108498098018010401"},"more_metadata6":"blahblahblah","more_metadata7":"blahblahblah","more_metadata8":"blahblahblah","more_metadata9":"blahblahblah","more_metadata10":"blahblahblah"}
{"system_metadata_you_dont_care_about":"blahblahblah","more_metadata1":"blahblahblah","more_metadata2":"blahblahblah","more_metadata3":"blahblahblah","more_metadata4":"blahblahblah","more_metadata5":"blahblahblah","doc":{"text":"********************HERE'S THE DATA YOU ACTUALLY WANT********************","id":92108498098018010401,"id_str":"92108498098018010401"},"more_metadata6":"blahblahblah","more_metadata7":"blahblahblah","more_metadata8":"blahblahblah","more_metadata9":"blahblahblah","more_metadata10":"blahblahblah"}
{"system_metadata_you_dont_care_about":"blahblahblah","more_metadata1":"blahblahblah","more_metadata2":"blahblahblah","more_metadata3":"blahblahblah","more_metadata4":"blahblahblah","more_metadata5":"blahblahblah","doc":{"text":"********************HERE'S THE DATA YOU ACTUALLY WANT********************","id":92108498098018010401,"id_str":"92108498098018010401"},"more_metadata6":"blahblahblah","more_metadata7":"blahblahblah","more_metadata8":"blahblahblah","more_metadata9":"blahblahblah","more_metadata10":"blahblahblah"}
```

Each line contains a single JSON object resembling the following:


```json
{
    "system_metadata_you_dont_care_about": "blahblahblah",
    "more_metadata1": "blahblahblah",
    "more_metadata2": "blahblahblah",
    "doc": {
        "text": "********************HERE'S THE DATA YOU ACTUALLY WANT********************",
        "id": 92108498098018010401,
        "id_str": "92108498098018010401",
        "_text": "********************HERE'S THE DATA YOU ACTUALLY WANT********************",
    },
    "more_metadata6": "blahblahblah",
    "more_metadata7": "blahblahblah"
}
```

The array structure is a complete dump of the file, which looks something like this:

```json
[
    {
        "_id": "e5daf1467d2438e31b11b44a82cbd7f5758ba5a1f1d3ecbcc6e1fc04dc9c7c4d-3016858092318",
        "_index": "org-77f135f331153568ab7eb0e4c24623a7-default-3769a33b9e88598e38317591e2ee31c3-default-030009",
        "_score": null,
        "_source": {
            "system_metadata_you_dont_care_about": "blahblahblah",
            "more_metadata1": "blahblahblah",
            "doc": {
                "text": "********************HERE'S THE DATA YOU ACTUALLY WANT********************",
                "id": 92108498098018010401,
                "id_str": "92108498098018010401",
                "_text": "********************HERE'S THE DATA YOU ACTUALLY WANT********************",
            },
            "more_metadata6": "blahblahblah"
        }
    },
    {
        "_id": "e5daf1467d2438e31b11b44a82cbd7f5758ba5a1f1d3ecbcc6e1fc04dc9c7c4d-3016858092318",
        "_index": "org-77f135f331153568ab7eb0e4c24623a7-default-3769a33b9e88598e38317591e2ee31c3-default-030009",
        "_score": null,
        "_source": {
            "system_metadata_you_dont_care_about": "blahblahblah",
            "more_metadata1": "blahblahblah",
            "doc": {
                "text": "********************HERE'S THE DATA YOU ACTUALLY WANT********************",
                "id": 92108498098018010401,
                "id_str": "92108498098018010401",
                "_text": "********************HERE'S THE DATA YOU ACTUALLY WANT********************",
            },
            "more_metadata6": "blahblahblah"
        }
    }
]
```

This has three unfortunate consequences:

1. Packages that were purpose-built to work directly with `{rtweet}`'s data frames can't play along with your data.
2. You're going to waste most of your time (and memory) getting data into R that you're not going to use.
3. The data are _very_ tedious to restructure in R (lists of lists of lists of lists of lists...).

`{tweetio}` solves this by parsing everything and building the data frames at the C++ level.



# Spatial Tweets


If you have `{sf}` installed, you can use `as_tweet_sf()` to only keep those tweets that contain valid bounding box polygons.

```{r}
tweet_sf <- big_tweet_df %>% 
  as_tweet_sf()

tweet_sf[, "bbox_coords"] %>% head()
```


There are currently three columns that can potentially hold bounding boxes:

1. `bbox_coords`
2. `quoted_bbox_coords`
3. `retweet_bbox_coords`

You can select which one to use to build your `sf` object by modifying the `geom_col=` parameter (default: `"bbox_coords"`)

```{r}
big_tweet_df %>% 
  as_tweet_sf(geom_col = "quoted_bbox_coords") %>% 
  .[, "quoted_bbox_coords"] %>% 
  head()
```

You can also build _all_ the supported bounding boxes by setting `geom_col=` to `"all"`.

```{r}
all_bboxes <- big_tweet_df %>% 
  as_tweet_sf(geom_col = "all")

all_bboxes[, c("geometry", "which_geom")] %>% 
  unique(by = "which_geom")
```


From there, you can easily use the data like any other `{sf}` object.


```{r, fig.width=12, fig.height=8}
library(ggplot2)

world <- rnaturalearth::ne_countries(returnclass = "sf")
world <- world[world$continent != "Antarctica", ]

sf_for_gg <- sf::st_wrap_dateline(all_bboxes)

ggplot(sf_for_gg) +
  geom_sf(fill = "white", color = "lightgray", data = world) +
  geom_sf(aes(fill = which_geom, color = which_geom), alpha = 0.15, size = 1, show.legend = TRUE) +
  coord_sf(crs = 3857) +
  scale_fill_viridis_d() +
  scale_color_viridis_d() +
  theme(legend.title = element_blank(), legend.position = "top",
        panel.background = element_rect(fill = "#daf3ff"))
```

# Tweet Networks

If you want to analyze tweet networks, you can get started immediately using `tweetio::as_igraph()`.

```{r}
big_tweet_df %>%
  as_igraph()
```

If you want to take advantage of all the metadata available, you can set `all_status_data` and/or `all_user_data` to `TRUE`

```{r}
big_tweet_df %>% 
  as_igraph(all_user_data = TRUE, all_status_data = TRUE)
```


You can also build two-mode networks by specifying the `target_class` as `"hashtag"`s, `"url"`s, or `"media"`. The returned `<igraph>` will be set as bipartite following `{igraph}`'s convention of a `logical` vertex attribute specifying each's partition (users are `TRUE`).

```{r}
big_tweet_df %>% 
  as_igraph(target_class = "hashtag")
```

```{r}
big_tweet_df %>% 
  as_igraph(target_class = "url")
```

```{r}
big_tweet_df %>% 
  as_igraph(target_class = "media")
```

You're not stuck with going directly to `<igraph>` objects though. Underneath the hood, `as_igraph()` calls `as_proto_net()`, which builds the edge and node data frames first. `as_proto_net()` is also exported.

```{r}
big_tweet_df %>% 
  as_proto_net(all_status_data = TRUE, all_user_data = TRUE, as_tibble = TRUE) %>% 
  lapply(head)
```


# Progress

### Supported Data Inputs

- [x] Twitter API streams: .json, .json.gz
- [x] API to Elasticsearch data dump (JSON Array): .json, .json.gz
- [x] API to Elasticsearch data dump (line-delimited JSON): .jsonl, .jsonl.gz

### Supported Data Outputs

- [x] CSV
- [x] Excel
- [x] Gephi-friendly GraphML

### Structures

- [x] `{rtweet}`-style data frames
- [x] Spatial Tweets via `{sf}`
- [x] Tweet networks via `{igraph}`


# Shout Outs

 The [`{rtweet}`](https://rtweet.info/) package __spoils R users rotten__, in the best possible way. The underlying data carpentry is so seamless that the user doesn't need to know anything about the horrors of Twitter data, which is pretty amazing. If you use `{rtweet}`, you probably owe [Michael Kearney](https://twitter.com/kearneymw) some [citations](https://github.com/mkearney/rtweet_citations). If he hadn't developed a way to sensibly structure tweet data frames in R, `{tweetio}` would've never happened.
 

`{tweetio}` uses a combination of C++ via [`{Rcpp}`](http://www.rcpp.org/), the [`rapidjson`](http://rapidjson.org/) C++ library (made available by [`{rapidjsonr}`](https://cran.r-project.org/web/packages/rapidjsonr/index.html)), [`{jsonify}`](https://cran.r-project.org/web/packages/jsonify/index.html)) for an R-level interface to `rapidjson`, [`{RcppProgress}`](https://cran.r-project.org/web/packages/RcppProgress/index.html)), and __R's not-so-secret super weapon__: [`{data.table}`](https://rdatatable.gitlab.io/data.table/).

Major inspiration from [`{ndjson}`](https://gitlab.com/hrbrmstr/ndjson) was taken, particularly its use of [`Gzstream`](https://www.cs.unc.edu/Research/compgeom/gzstream/).



