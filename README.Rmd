---
output:
  github_document:
    html_preview: true
  html_document:
    keep_md: yes
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---



<!-- README.Rmd generates README.md. -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  # collapse = TRUE,
  fig.align = "center",
  comment = "#>",
  fig.path = "man/figures/",
  message = FALSE,
  warning = FALSE
)

# options(width = 400)
```


# `{tweetio}`

<!-- badges: start -->
[![Lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![GitHub last commit](https://img.shields.io/github/last-commit/knapply/tweetio.svg)](https://github.com/knapply/tweetio/commits/master)
[![Codecov test coverage](https://codecov.io/gh/knapply/tweetio/branch/master/graph/badge.svg)](https://codecov.io/gh/knapply/tweetio?branch=master)
[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/knapply/tweetio?branch=master&svg=true)](https://ci.appveyor.com/project/knapply/tweetio)
[![Travis-CI Build Status](https://travis-ci.org/knapply/tweetio.svg?branch=master)](https://travis-ci.org/knapply/tweetio)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![Depends](https://img.shields.io/badge/Depends-GNU_R>=3.6-blue.svg)](https://www.r-project.org/)
[![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/knapply/tweetio.svg)](https://github.com/knapply/tweetio)
[![HitCount](http://hits.dwyl.io/knapply/tweetio.svg)](http://hits.dwyl.io/knapply/tweetio)
<!-- badges: end -->


# Introduction

__The [`{rtweet}`](https://rtweet.info/) package spoils R users rotten__. The underlying data carpentry is so seamless that the user doesn't need to know anything about the horrors of Twitter data, which is pretty amazing. If you use `{rtweet}`, you owe Mike Kearney some serious gratitude (_and citations_).

`{tweetio}` is __not__ a competitor to `{rtweet}`. It only focuses on one thing: __going from raw tweets to `{rtweet}`-style data frames as fast as possible__, whether the data came from the Twitter APIs, a vendor, or some other source. You bring the data, `{tweetio}` gets it into R.

To accomplish this, `{tweetio}` uses a combination of C++ via `{Rcpp}`, the `rapidjson` C++ library (made available by `{rapidjsonr}`), and `{data.table}`.

Major inspiration from {`ndjson`} was taken, including its handling of gzipped-files. Bulk processing of multiple files is handled by `{future}` and `{furrr}`, if available.


# Installation

You'll need a C++ compiler. If you're using Windows, that means [Rtools](https://cran.r-project.org/bin/windows/Rtools/).

```{r, eval=FALSE}
# install.packages("remotes")
remotes::install_github("knapply/tweetio")
```

# Usage

First, we'll parse a stream obtained by `rtweet::stream_tweets()` that has been compressed to a `json.gz` file.

```{r}
library(tweetio)
library(purrr)

path_to_rtweet_stream <- "inst/example-data/api-stream.json.gz"
path_to_rtweet_stream %>% file.size() %>% scales::number_bytes() # 153 Mb before compression
```


```{r}
rtweet_time <- system.time(
  rtweet_parsed <- read_tweets(path_to_rtweet_stream, type = "normal")
); rtweet_time
rtweet_parsed %>% dim() %>% scales::comma() %>% set_names(c("# rows", "# columns"))
```

`r rtweet_time[["elapsed"]]` seconds for `r scales::comma(nrow(rtweet_parsed))` tweets ain't too shabby.


How about some vendor data?

```{r, echo=FALSE}
path_to_vendor_data <- readRDS("path_to_vendor_data.rds")
```


```{r}
all_vendor_files <- dir(path_to_vendor_data, pattern = "\\.jsonl\\.gz$",
                        full.names = TRUE)

all_vendor_files[[1L]] %>% file.size() %>% scales::number_bytes() # 1.6 Gb before compression
```

```{r}
single_vendor_time <- system.time(
  single_vendor_parsed <- read_tweets(all_vendor_files[[1L]], 
                                      type = "nested_doc")
); single_vendor_time
single_vendor_parsed %>% dim() %>% scales::comma() %>% set_names(c("# rows", "# columns"))
```


`r single_vendor_time[["elapsed"]]` seconds for `r scales::comma(nrow(single_vendor_parsed))` tweets seems pretty fast.


How about bulk data? While maybe not "big" data, handling millions of lines of JSON in R isn't exactly a picnic.

```{r}
all_vendor_files %>% map_dbl(file.size) %>% sum() %>% scales::number_bytes() # 47 Gb before compression
system.time(
  bulk_parsed <- read_tweets(all_vendor_files, type = "nested_doc", 
                             .progress = FALSE)
)
bulk_parsed %>% dim() %>% scales::comma() %>% set_names(c("# rows", "# columns"))
```

Nearly 3,000,000 tweets in a few minutes? Until Rtools 40 hits (or the simdjson library decides to relax its C++17 requirement), I'm not certain we can go much faster.



