---
output:
  github_document:
    html_preview: true
  html_document:
    keep_md: yes
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---



<!-- README.Rmd generates README.md. -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  # collapse = TRUE,
  fig.align = "center",
  comment = "#>",
  fig.path = "man/figures/",
  message = FALSE,
  warning = FALSE
)

# options(width = 400)
```


# `{tweetio}`

<!-- badges: start -->
[![Lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![GitHub last commit](https://img.shields.io/github/last-commit/knapply/tweetio.svg)](https://github.com/knapply/tweetio/commits/master)
[![Codecov test coverage](https://codecov.io/gh/knapply/tweetio/branch/master/graph/badge.svg)](https://codecov.io/gh/knapply/tweetio?branch=master)
[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/knapply/tweetio?branch=master&svg=true)](https://ci.appveyor.com/project/knapply/tweetio)
[![Travis-CI Build Status](https://travis-ci.org/knapply/tweetio.svg?branch=master)](https://travis-ci.org/knapply/tweetio)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![Depends](https://img.shields.io/badge/Depends-GNU_R>=3.3-blue.svg)](https://www.r-project.org/)
[![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/knapply/tweetio.svg)](https://github.com/knapply/tweetio)
[![HitCount](http://hits.dwyl.io/knapply/tweetio.svg)](http://hits.dwyl.io/knapply/tweetio)
<!-- badges: end -->


# Introduction

__The [`{rtweet}`](https://rtweet.info/) package spoils R users _rotten_... in the best possible way__. The behind-the-scenes data carpentry is so seamless that the user doesn't need to know anything about the horrors of Twitter data, which is pretty amazing. If you use `{rtweet}`, you owe Mike Kearney some serious gratitude/citations.

`{tweetio}` is __not__ a competitor to `{rtweet}`, but it definitely attempts to emulate its data frame schema because...

1. It's incredibly easy to use.
2. It's more efficient to analyze than a key-value format following the raw data.
3. It'd be a waste not to maximize compatibiltiy with tools built specifically around `{rtweet}`'s data frames.

> ___You__ bring the data, `{tweetio}` gets them into R._

`{tweetio}` focuses on one thing: __going from raw tweets to `{rtweet}`-style data frames (or other useful structures) as quickly and efficiently as possible__. Whether the data came from the Twitter API, a vendor, or some other source, `{tweetio}`'s job is to get them into R.

## Installation

You'll need a C++ compiler. If you're using Windows, that means [Rtools](https://cran.r-project.org/bin/windows/Rtools/).

```{r, eval=FALSE}
# install.packages("remotes")
remotes::install_github("knapply/tweetio")
```

## Supported Data Inputs

- [x] Twitter API streams: .json, .json.gz
- [x] API to Elasticsearch data dump (JSON Array): .json, .json.gz
- [x] API to Elasticsearch data dump (line-delimited JSON): .jsonl, .jsonl.gz


## Supported Data Outputs

- [ ] CSV
- [ ] .graphml via [`{tweetgraph}`](https://knapply.github.io/tweetgraph/) integration



## Usage

### Simple Example

First, we'll save a stream of tweets using `rtweet::stream_tweets()`.

```{r}
temp_file <- tempfile(fileext = ".json")
rtweet::stream_tweets(timeout = 15, parse = FALSE, file_name = temp_file)
```

We can then pass the file path to `tweetio::read_tweets()` to efficiently parse the data into an `{rtweet}`-style data frame.

```{r}
library(tweetio)

small_rtweet_stream <- read_tweets(temp_file)
dplyr::glimpse(small_rtweet_stream)
```

### Scaling Up

In reality, we're more interested in handling larger data sets. Here's a larger file obtained using `rtweet::stream_tweets()` that has been compressed to an 18 Mb .gz file (153 Mb uncompressed).

```{r}
rtweet_stream_path <- "inst/example-data/api-stream.json.gz"
big_rtweet_stream <- read_tweets(rtweet_stream_path)

tibble::as_tibble(big_rtweet_stream)
```

How long does that take?

```{r}
library(microbenchmark)

big_rtweet_time <- microbenchmark(
  rtweet_stream = read_tweets(rtweet_stream_path),
  times = 3
)

big_rtweet_time
```


### Vendor Data

Vendors seem to love storing social media data in Elasticsearch databases, meaning that the data collected from APIs is stored inside a nested JSON object named `"doc"`. `"doc"` is typically embedded in a ton of system metadata, most of which you probably don't care about.

This has three unfortunate consequences:

1. Packages that were purpose-built to work directly with the Twitter API aren't going to play along with your data.
2. You're going to waste most of your time (and memory) getting data into R that you're not going to use.
3. The data are _very_ tedious to restructure in R (lists of lists of lists of lists of lists...).

`{tweetio}` solves this by parsing everything at the C++ level, but only returning the actual tweet data to R.

```{r, echo=FALSE}
path_to_vendor_data <- readRDS("path_to_vendor_data.rds")
all_vendor_files <- dir(path_to_vendor_data, 
                        pattern = "-full-\\d{4}\\.jsonl\\.gz$", 
                        full.names = TRUE)

single_vendor_file <- all_vendor_files[[1L]]
single_vendor_file_size <- file.size(single_vendor_file)

library(scales)
number_bytes(single_vendor_file_size)
```


```{r}
single_vendor_time <- microbenchmark(

  single_vendor_parsed <- read_tweets(single_vendor_file) # ********************
  
  , times = 1
)

single_vendor_time
```


### Bulk Processing

While maybe not "big" data, handling millions of lines of JSON spread across dozens of files in R isn't exactly a picnic, but `read_tweets_bulk()` attempts to make this as easy as possible.

We can run `read_tweets_bulk()` either sequentially or in parallel. By setting `in_parallel=` to `FALSE`, it will always run sequentially, processing each file one at a time with `lapply()` before collapsing the resulting data frames via `data.table::rbindlist()`.


```{r}
sequential_bulk_files <- all_vendor_files[1:4]
sequential_bulk_file_size <- sum(sapply(sequential_bulk_files, file.size))

number_bytes(sequential_bulk_file_size)
```

```{r}
sequential_bulk_time <- microbenchmark(
  
  sequential_bulk_parsed <- read_tweets_bulk(sequential_bulk_files, # **********
                                             in_parallel = FALSE)   # **********
  
  , times = 1
)

sequential_bulk_time
```


If `in_parallel=` is set to `TRUE` (the default) _and_ `{future}` and `{future.apply}` are available, `read_tweets_bulk()` can be run in parallel via `future.apply::future_lapply()`.

```{r}
parallel_bulk_file_size <- sum(sapply(all_vendor_files, file.size))

number_bytes(parallel_bulk_file_size)
```

```{r}
parallel_bulk_time <- microbenchmark(
  
  read_tweets_bulk(all_vendor_files) # *****************************************
  
  , times = 1
)

parallel_bulk_time
```


## Benchmarks

```{r}
res <- microbenchmark(
  read_tweets(all_vendor_files[[1]]),
  read_tweets(all_vendor_files[[2]]),
  read_tweets(all_vendor_files[[3]]),
  read_tweets(all_vendor_files[[4]]),
  read_tweets(all_vendor_files[[5]]),
  read_tweets(all_vendor_files[[6]]),
  read_tweets(all_vendor_files[[7]]),
  read_tweets(all_vendor_files[[8]]),
  read_tweets(all_vendor_files[[9]]),
  read_tweets(all_vendor_files[[10]]),
  read_tweets(all_vendor_files[[11]]),
  read_tweets(all_vendor_files[[12]]),
  read_tweets(all_vendor_files[[13]]),
  read_tweets(all_vendor_files[[14]]),
  read_tweets(all_vendor_files[[15]]),
  
  times = 3
)

library(tidyverse)

bench_marks <- res %>% 
  as_tibble() %>% 
  mutate(file_size = expr %>% 
           str_extract("(?<=\\().*(?=\\)$)") %>% 
           map_chr(~ eval(parse(text = .x))) %>% 
           file.size()
           ) %>% 
  mutate(time = time / 1e9)

bench_marks %>% 
  ggplot(aes(x = file_size, y = time, color = factor(file_size))) +
  ggbeeswarm::geom_quasirandom() +
  guides(color = FALSE, size = FALSE) +
  labs(x = ".jsonl.gz File Size", y = "seconds") +
  scale_x_continuous(
    labels = function(.x) number_bytes(.x, symbol = "Mb", big.mark = ",")
  ) +
  coord_flip() +
  theme_minimal(base_size = 14, base_family = "serif") +
  theme(legend.position = "bottom")
```


Until Rtools 4.0 hits (or the [`simdjson`](https://github.com/lemire/simdjson) library decides to [relax its C++17 requirement](https://github.com/lemire/simdjson/issues/307)), I'm not sure how we can go much faster while maintaining cross-platform compatibility. That said, if C++ is your mother tongue (and you see room for optimization), please don't hesitate to contribute.



## Acknowledgements

`{tweetio}` uses a combination of C++ via [`{Rcpp}`](http://www.rcpp.org/), the [`rapidjson`](http://rapidjson.org/) C++ library (made available by [`{rapidjsonr}`](https://cran.r-project.org/web/packages/rapidjsonr/index.html)), and __R's secret weapon__: [`{data.table}`](https://rdatatable.gitlab.io/data.table/).

Major inspiration from [{`ndjson`}](https://gitlab.com/hrbrmstr/ndjson) was taken, including its use of [`Gzstream`](https://www.cs.unc.edu/Research/compgeom/gzstream/).
