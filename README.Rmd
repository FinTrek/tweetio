---
output:
  github_document:
    html_preview: true
  html_document:
    keep_md: yes
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---



<!-- README.Rmd generates README.md. -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  # collapse = TRUE,
  fig.align = "center",
  comment = "#>",
  fig.path = "man/figures/",
  message = FALSE,
  warning = FALSE
)

# options(width = 400)
```


# `{tweetio}`

<!-- badges: start -->
[![Lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![GitHub last commit](https://img.shields.io/github/last-commit/knapply/tweetio.svg)](https://github.com/knapply/tweetio/commits/master)
[![Codecov test coverage](https://codecov.io/gh/knapply/tweetio/branch/master/graph/badge.svg)](https://codecov.io/gh/knapply/tweetio?branch=master)
[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/knapply/tweetio?branch=master&svg=true)](https://ci.appveyor.com/project/knapply/tweetio)
[![Travis-CI Build Status](https://travis-ci.org/knapply/tweetio.svg?branch=master)](https://travis-ci.org/knapply/tweetio)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![Depends](https://img.shields.io/badge/Depends-GNU_R>=3.6-blue.svg)](https://www.r-project.org/)
[![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/knapply/tweetio.svg)](https://github.com/knapply/tweetio)
[![HitCount](http://hits.dwyl.io/knapply/tweetio.svg)](http://hits.dwyl.io/knapply/tweetio)
<!-- badges: end -->


# Introduction

__The [`{rtweet}`](https://rtweet.info/) package spoils R users _rotten_, in the best possible way__. The behind-the-scenes data carpentry is so seamless that the user doesn't need to know anything about the horrors of Twitter data, which is pretty amazing. If you use `{rtweet}`, you owe Mike Kearney some serious gratitude/citations.

`{tweetio}` is __not__ a competitor to `{rtweet}`, but it definitely attempts to emulate its data frame schema because...

1. It's incredibly easy to use.
2. It'd be a waste not to maximize compatibiltiy with tools built specifically around `{rtweet}`'s data frames.

<center>
  <h4>
      You bring the data, `{tweetio}` gets it into R.
  </h4>
</center>

`{tweetio}` only focuses on one thing: __going from raw tweets to `{rtweet}`-style data frames (or other useful structures) as quickly and efficiently as possible__. Whether the data came from the Twitter API, a vendor, or some other source, `{tweetio}`'s job is to get into R.

## Installation

You'll need a C++ compiler. If you're using Windows, that means [Rtools](https://cran.r-project.org/bin/windows/Rtools/).

```{r, eval=FALSE}
# install.packages("remotes")
remotes::install_github("knapply/tweetio")
```

## Usage

### Simple Example

First, we'll save a stream of tweets using `rtweet::stream_tweets()`. We'll set `timeout=` to `15` seconds here, but in a real scenario you would likely be streaming for _much_ longer.

```{r}
temp_file <- tempfile(fileext = ".json")
rtweet::stream_tweets(timeout = 15, parse = FALSE, file_name = temp_file)
```

We can then pass the file path to `tweetio::read_tweets()` to efficiently parse the data into an `{rtweet}`-style data frame.

```{r}
library(tweetio)

small_rtweet_stream <- read_tweets(temp_file)
dplyr::glimpse(small_rtweet_stream)
```


### Scaling Up

In reality, we're more interested in handling larger data sets. Here's a larger file obtained using `rtweet::stream_tweets()` that has been compressed to an 18 Mb .gz file (153 Mb uncompressed).

```{r}
big_rtweet_stream <- read_tweets("inst/example-data/api-stream.json.gz")
dim(big_rtweet_stream)
```

That's `r scales::comma(nrow(big_rtweet_stream))` tweets, but how long does that take?

```{r}
big_rtweet_time <- microbenchmark::microbenchmark(
  ".153" = read_tweets("inst/example-data/api-stream.json.gz"),
  times = 3
)
big_rtweet_time
```


### Vendor Data

It seems that the trend for third-party data vendors is to embed what the APIs provide inside a JSON object named `"doc"`, which is typically embedded in a ton of platform metadata (that you probably don't care about).

This has three unfortunate consequences:

1. Packages that were purpose-built to work directly with the Twitter API aren't going to play along with your data.
2. You're going to waste most of your time and computational resources on getting data into R that you're not going to use.
3. The data are _very_ tedious to restructure in R (lists of lists of lists of lists of lists...).

`{tweetio}` solves this by parsing everything at the C++ level, but only returning the actual tweet data to R. Here's an example.

```{r, echo=FALSE}
path_to_vendor_data <- readRDS("path_to_vendor_data.rds")
all_vendor_files <- dir(path_to_vendor_data, pattern = "\\.jsonl\\.gz$",
                        full.names = TRUE)

# 173 Mb, 1.6 Gb uncompressed
single_vendor_file <- all_vendor_files[[1L]] 
```

The only difference between reading data following this nested `"doc"` convention and `"normal"` data is that the user must set `type=` to `"nested_doc"`. This will likely be simplified to auto-detect the data's structure in the future.

```{r}
single_vendor_parsed <- read_tweets(single_vendor_file, type = "nested_doc")
dim(single_vendor_parsed)
```

That's `r scales::comma(nrow(single_vendor_parsed))`, but about how long does that take?

```{r}
single_vendor_time <- microbenchmark::microbenchmark(
  "1.6" = read_tweets(single_vendor_file, type = "nested_doc"),
  times = 3
)
single_vendor_time
```


### Bulk Processing

While maybe not "big" data, handling millions of lines of JSON spread across dozens of files in R isn't exactly a picnic. To handle this situation, the current implementation uses `furrr::future_map()` to process each file in parallel before collapsing the resulting data frames via `data.table::rbindlist()`.

`all_vendor_files` consists of 6 Gb (47 Gb uncompressed) spread across 30 files, which we can still process using `tweetio::read_tweets()`.

```{r}
bulk_parsed <- read_tweets(all_vendor_files, type = "nested_doc", 
                           .progress = FALSE)
dim(bulk_parsed)
```


How long does that take?

```{r}
bulk_time <- microbenchmark::microbenchmark(
  "47" = read_tweets(all_vendor_files, type = "nested_doc", 
                           .progress = FALSE),
  times = 1
)
bulk_time
```


## Benchmarks

```{r}
res <- do.call(rbind, list(big_rtweet_time, single_vendor_time, bulk_time))

library(dplyr)
library(ggplot2)

res %>% 
  mutate(file_size = paste(expr, "Gb"),
         time = time / 1e+09,
         `parallel?` = expr == "47") %>% 
  ggplot(aes(x = file_size, y = time, 
             color = file_size, size = as.numeric(expr), shape = `parallel?`)) +
  ggbeeswarm::geom_quasirandom() +
  scale_y_log10() +
  guides(color = FALSE, size = FALSE) +
  labs(x = "Size (uncompressed)", y = "seconds") +
  coord_flip() +
  theme_minimal(base_size = 14, base_family = "serif") +
  theme(legend.position = "bottom")
```


Until Rtools 4.0 hits (or the [`simdjson`](https://github.com/lemire/simdjson) library decides to [relax its C++17 requirement](https://github.com/lemire/simdjson/issues/307)), I'm not sure how we can go much faster while maintaining cross-platform compatibility. That said, if C++ is your mother tongue (and you see room for optimization), please don't hesitate to contribute.



## Acknowledgements

`{tweetio}` uses a combination of C++ via [`{Rcpp}`](http://www.rcpp.org/l), the [`rapidjson`](http://rapidjson.org/) C++ library (made available by [`{rapidjsonr}`](https://cran.r-project.org/web/packages/rapidjsonr/index.html)), and __R's secret weapon__: [`{data.table}`](https://rdatatable.gitlab.io/data.table/).

Major inspiration from [{`ndjson`}](https://gitlab.com/hrbrmstr/ndjson) was taken, including its use of [`Gzstream`](https://www.cs.unc.edu/Research/compgeom/gzstream/).

Parallel processing of multiple files is (currently) handled by [`{future}`](https://cran.r-project.org/web/packages/future/index.html) and [`{furrr}`](https://davisvaughan.github.io/furrr/), if available.

