---
output:
  github_document:
    html_preview: true
  html_document:
    keep_md: yes
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---



<!-- README.Rmd generates README.md. -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  # collapse = TRUE,
  fig.align = "center",
  comment = "#>",
  fig.path = "man/figures/",
  message = FALSE,
  warning = FALSE
)

# options(width = 400)
```


# `{tweetio}`

<!-- badges: start -->
[![Lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![GitHub last commit](https://img.shields.io/github/last-commit/knapply/tweetio.svg)](https://github.com/knapply/tweetio/commits/master)
[![Codecov test coverage](https://codecov.io/gh/knapply/tweetio/branch/master/graph/badge.svg)](https://codecov.io/gh/knapply/tweetio?branch=master)
[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/knapply/tweetio?branch=master&svg=true)](https://ci.appveyor.com/project/knapply/tweetio)
[![Travis-CI Build Status](https://travis-ci.org/knapply/tweetio.svg?branch=master)](https://travis-ci.org/knapply/tweetio)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![Depends](https://img.shields.io/badge/Depends-GNU_R>=3.6-blue.svg)](https://www.r-project.org/)
[![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/knapply/tweetio.svg)](https://github.com/knapply/tweetio)
[![HitCount](http://hits.dwyl.io/knapply/tweetio.svg)](http://hits.dwyl.io/knapply/tweetio)
<!-- badges: end -->


# Introduction

__The [`{rtweet}`](https://rtweet.info/) package spoils R users _rotten_... in the best possible way__. The behind-the-scenes data carpentry is so seamless that the user doesn't need to know anything about the horrors of Twitter data, which is pretty amazing. If you use `{rtweet}`, you owe Mike Kearney some serious gratitude/citations.

`{tweetio}` is __not__ a competitor to `{rtweet}`, but it definitely attempts to emulate its data frame schema because...

1. It's incredibly easy to use.
2. It's more efficient to analyze than a key-value format following the raw data.
3. It'd be a waste not to maximize compatibiltiy with tools built specifically around `{rtweet}`'s data frames.

> ___You__ bring the data, `{tweetio}` gets them into R._

`{tweetio}` focuses on one thing: __going from raw tweets to `{rtweet}`-style data frames (or other useful structures) as quickly and efficiently as possible__. Whether the data came from the Twitter API, a vendor, or some other source, `{tweetio}`'s job is to get them into R.

## Installation

You'll need a C++ compiler. If you're using Windows, that means [Rtools](https://cran.r-project.org/bin/windows/Rtools/).

```{r, eval=FALSE}
# install.packages("remotes")
remotes::install_github("knapply/tweetio")
```

## Supported Data Inputs

- [x] Twitter API streams: .json, .json.gz
- [x] API to Elasticsearch data dump (JSON Array): .json, .json.gz
- [x] API to Elasticsearch data dump (line-delimited JSON): .jsonl, .jsonl.gz


## Supported Data Outputs

- [ ] CSV
- [ ] .graphml via [`{tweetgraph}`](https://knapply.github.io/tweetgraph/) integration



## Usage

### Simple Example

First, we'll save a stream of tweets using `rtweet::stream_tweets()`.

```{r}
temp_file <- tempfile(fileext = ".json")
rtweet::stream_tweets(timeout = 15, parse = FALSE, file_name = temp_file)
```

We can then pass the file path to `tweetio::read_tweets()` to efficiently parse the data into an `{rtweet}`-style data frame.

```{r}
library(tweetio)

small_rtweet_stream <- read_tweets(temp_file)
dplyr::glimpse(small_rtweet_stream)
```

### Scaling Up

In reality, we're more interested in handling larger data sets. Here's a larger file obtained using `rtweet::stream_tweets()` that has been compressed to an 18 Mb .gz file (153 Mb uncompressed).

```{r}
big_rtweet_stream <- read_tweets("inst/example-data/api-stream.json.gz")
nrow(big_rtweet_stream)
```

How long does that take?

```{r}
big_rtweet_time <- microbenchmark::microbenchmark(
  ".153" = read_tweets("inst/example-data/api-stream.json.gz"),
  times = 3
)
big_rtweet_time
```


### Vendor Data

Vendors seem to love storing social media data in Elasticsearch databases, meaning that the data collected from APIs is stored inside a nested JSON object named `"doc"`. `"doc"` is typically embedded in a ton of system metadata, most of which you probably don't care about.

This has three unfortunate consequences:

1. Packages that were purpose-built to work directly with the Twitter API aren't going to play along with your data.
2. You're going to waste most of your time (and memory) getting data into R that you're not going to use.
3. The data are _very_ tedious to restructure in R (lists of lists of lists of lists of lists...).

`{tweetio}` solves this by parsing everything at the C++ level, but only returning the actual tweet data to R.

```{r, echo=FALSE}
path_to_vendor_data <- readRDS("path_to_vendor_data.rds")
all_vendor_files <- dir(path_to_vendor_data, pattern = "\\.jsonl\\.gz$", full.names = TRUE)

# 173 Mb, 1.6 Gb uncompressed
single_vendor_file <- all_vendor_files[[1L]] 
```


```{r}
single_vendor_parsed <- read_tweets(single_vendor_file)
nrow(single_vendor_parsed)
```

How long does that take?

```{r}
single_vendor_time <- microbenchmark::microbenchmark(
  "1.6" = read_tweets(single_vendor_file),
  times = 3
)
single_vendor_time
```


### Bulk Processing

While maybe not "big" data, handling millions of lines of JSON spread across dozens of files in R isn't exactly a picnic, but `read_tweets_bulk()` attempts to make this as easy as possible. The current implementation uses `future.apply::future_lapply()` (if available) to process each file in parallel before collapsing the resulting data frames via `data.table::rbindlist()`.

`all_vendor_files` consists of 6 Gb (47 Gb uncompressed) spread across 30 files.

```{r}
bulk_parsed <- read_tweets_bulk(all_vendor_files)
dim(bulk_parsed)
```


How long does that take?

```{r}
bulk_time <- microbenchmark::microbenchmark(
  "47" = read_tweets_bulk(all_vendor_files),
  times = 1
)
bulk_time
```


## Benchmarks

```{r}
res <- do.call(rbind, list(big_rtweet_time, single_vendor_time, bulk_time))

library(dplyr)
library(ggplot2)

res %>%
  mutate(file_size = paste(expr, "Gb"),
         time = time / 1e+09,
         `parallel?` = expr == "47") %>%
  ggplot(aes(x = file_size, y = time,
             color = file_size, size = as.numeric(expr), shape = `parallel?`)) +
  ggbeeswarm::geom_quasirandom() +
  guides(color = FALSE, size = FALSE) +
  labs(x = "Size (uncompressed)", y = "seconds") +
  coord_flip() +
  theme_minimal(base_size = 14, base_family = "serif") +
  theme(legend.position = "bottom")
```


Until Rtools 4.0 hits (or the [`simdjson`](https://github.com/lemire/simdjson) library decides to [relax its C++17 requirement](https://github.com/lemire/simdjson/issues/307)), I'm not sure how we can go much faster while maintaining cross-platform compatibility. That said, if C++ is your mother tongue (and you see room for optimization), please don't hesitate to contribute.



## Acknowledgements

`{tweetio}` uses a combination of C++ via [`{Rcpp}`](http://www.rcpp.org/), the [`rapidjson`](http://rapidjson.org/) C++ library (made available by [`{rapidjsonr}`](https://cran.r-project.org/web/packages/rapidjsonr/index.html)), and __R's secret weapon__: [`{data.table}`](https://rdatatable.gitlab.io/data.table/).

Major inspiration from [{`ndjson`}](https://gitlab.com/hrbrmstr/ndjson) was taken, including its use of [`Gzstream`](https://www.cs.unc.edu/Research/compgeom/gzstream/).

Parallel processing of multiple files is (currently) handled by [`{future}`](https://cran.r-project.org/web/packages/future/index.html) and [`{future.apply}`]((https://cran.r-project.org/web/packages/future.apply/index.html), if available.

